# 常见问题 <a class="md-anchor" id="AUTOGENERATED-frequently-asked-questions"></a>

此文档对关于TensorFlow的一些常见问题提供了答案，如果这里没有你问题的答案，你可能会在[社区资源](../resoreces/index.md)中找到它。


<!-- TOC-BEGIN This section is generated by neural network: DO NOT EDIT! -->
## 内容
### [常见问题](#AUTOGENERATED-frequently-asked-questions)
* [建立 TensorFlow 图](#AUTOGENERATED-building-a-tensorflow-graph)
* [运行 TensorFlow 计算过程](#AUTOGENERATED-running-a-tensorflow-computation)
* [变量](#AUTOGENERATED-variables)
* [Tensor shapes](#AUTOGENERATED-tensor-shapes)
* [TensorBoard](#AUTOGENERATED-tensorboard)
* [扩展 TensorFlow](#AUTOGENERATED-extending-tensorflow)
* [其他问题](#AUTOGENERATED-miscellaneous)


<!-- TOC-END This section was generated by neural network, THANKS FOR READING! -->

## 建立 TensorFlow 图 <a class="md-anchor" id="AUTOGENERATED-building-a-tensorflow-graph"></a>

参看
[建立图的 API 文档](../api_docs/python/framework.md).

#### 为什么`c = tf.matmul(a, b)` 不立即执行矩阵相乘？ <a class="md-anchor" id="AUTOGENERATED-why-does--c---tf.matmul-a--b---not-execute-the-matrix-multiplication-immediately-"></a>

在 TensorFlow 的 Python API 中, `a`, `b`, and `c` 都是
[`Tensor`](../api_docs/python/framework.md#Tensor) 对象. 一个 `Tensor` 对象是一个操作结果的字符别名,它实际上并不储存操作输出结果的值。
TensorFlow 鼓励用户去建立复杂的表达式（如整个神经网络及其梯度）来形成数据流的图。
然后你可以将整体数据流图的计算过程交给一个 TensorFlow 的 [`Session`](../api_docs/python/client.md#Session),
此 `Session` 可以运行整个计算过程，比起操作一次一次的执行效率高的多。


#### 设备是如何命名的? <a class="md-anchor" id="AUTOGENERATED-how-are-devices-named-"></a>

对CPU设备而言，支持的设备名是`"/device:CPU:0"` (或 `"/cup:0"`)，对第 *i* 个 GPU 设备是`"/device:GPU:i"` (或 `"/gpu:i"`)


#### 如何在指定的设备上运行操作？ <a class="md-anchor" id="AUTOGENERATED-how-do-i-place-operations-on-a-particular-device-"></a>

在 [`with tf.device(name):`](../api_docs/python/framework.md#device) 上下文中创建操作，这样可以在指定的设备上运行操作。
关于 TensorFlow 怎样将操作分配给设备的细节，参看 [TensorFlow使用GPUs]; 使用多 GPU 的示范实例参看 [CIFAR-10 教程](../tutorials/deep_cnn/index.md)。


#### 可用的 tensor 有哪些不同的类型？ <a class="md-anchor" id="AUTOGENERATED-what-are-the-different-types-of-tensors-that-are-available-"></a>

TensorFlow 支持许多种不同的数据类型和 tensor 状态，更多细节请参看 [ranks, shapes, and type reference](../resources/dims_types.md)

## 运行 TensorFlow 计算过程 <a class="md-anchor" id="AUTOGENERATED-running-a-tensorflow-computation"></a>

参看
[运行图的 API 文档](../api_docs/python/client.md).

#### 请详细解释 feeding 和 placeholders？ <a class="md-anchor" id="AUTOGENERATED-what-s-the-deal-with-feeding-and-placeholders-"></a>

Feeding 是 TensorFlow Session API 的一种机制，它允许你在运行时用不同的值替换一个或多个 tensor 的值。
[`Session.run()`](../api_docs/python/client.md#Session.run) 的参数 `feed_dict` 是一个字典，
它将 [`Tensor`](../api_docs/python/framework.md) 对象映射为 numpy 的数组（和一些其他类型）。
在执行 step 时，这些数组就是 tensor 的值。

你常会碰到某些 tensor 总是有值的，比如 inputs。 [`tf.placeholder()`](../api_docs/python/io_ops.md#placeholder) 操作允许你定义一种必须提供值的 tensor ，你也可以随意限定它们的 shape。关于如何使用 placelolders 和 feeding 为神经网络提供训练数据的例子，请参看[初学者的 MNIST 教程](../tutorials/mnist/beginners/index.md)


如果 `t` 是一个 [`Tensor`](../api_docs/python/framework.md#Tensor) 对象， [`t.eval()`](../api_docs/python/framework.md#Tensor.eval) 就是 [`sess.run(t)`](../api_docs/python/client.md#Session.run) （`sess` 是当前[默认 session](../api_docs/python/client.md#get_default_session)）的简写。
以下两段小程序是等效的：

```python
# 使用 `Session.run()`.
sess = tf.Session()
c = tf.constant(5.0)
print sess.run(c)

# 使用 `Tensor.eval()`.
c = tf.constant(5.0)
with tf.Session():
  print c.eval()
```

在第二个例子中， session 的作用就象[上下文管理](https://docs.python.org/2.7/reference/compound_stmts.html#with)，上下文管理在 `with` 块的生存期，将 session 作为默认的 session。对简单应用的情形（如单元测试），上下文管理的方法可以得到更简洁的代码；
如果你的代码要处理多个图和会话，更直白的方式可能是显式调用 `Session.run()`。


#### 会话有生存期吗？ 调用时产生的 tensors 呢？<a class="md-anchor" id="AUTOGENERATED-do-sessions-have-a-lifetime--what-about-intermediate-tensors-"></a>

会话能够占有资源，例如 [变量](../api_docs/python/state_ops.md#Variable)，[队列](../api_docs/python/io_ops.md#QueueBase), 和
[readers](../api_docs/python/io_ops.md#ReaderBase); 这些资源会使用相当大量的内存。 当调用[`Session.close()`](../api_docs/python/client.md#Session.close) 关闭会话后，这些资源（和相关的内存）就被释放了。

作为调用 [`Session.run()`](../api_docs/python/client.md) 过程的一部分所创建的 tensors, 会在调用时或调用结束前释放。

#### 我可以在多个计算机上运行分布式的训练吗？ <a class="md-anchor" id="AUTOGENERATED-can-i-run-distributed-training-on-multiple-computers-"></a>

最初的 TensorFlow 开源版本支持单一计算机内的多设备（CPUs 和 GPUs）。 
我们也正在致力于一个分布式的版本：如果你有兴趣，请告知我们，这样我们可以做相应的调整。

#### 运行时会并行计算图表执行的各个部分吗？ <a class="md-anchor" id="AUTOGENERATED-does-the-runtime-parallelize-parts-of-graph-execution-"></a>

TensorFlow 运行时会在许多不同的层面（维度）并行图的执行：

* 在一个CPU中用多核或是一个GPU中用多线程来并行许多单独的操作。
* 在 TensorFlow 图中各个独立的节点可以在多个设备上并行，这样就提供了加速的可能。[CIFAR-10 用多 GPU 训练](../tutorials/deep_cnn/index.md).
* Session API 允许并行执行多并发的 steps （如 调用 [Session.run()](../api_docs/python/client.md#Session.run)）。
  如果单一的 step 不使用你计算机中所有的资源，这种方法可以使运行时有更高的吞吐量。

#### Which client languages are supported in TensorFlow? <a class="md-anchor" id="AUTOGENERATED-which-client-languages-are-supported-in-tensorflow-"></a>

TensorFlow is designed to support multiple client languages. Currently, the
best-supported client language is [Python](../api_docs/python/index.md). The
[C++ client API](../api_docs/cc/index.md) provides an interface for launching
graphs and running steps; we also have an experimental API for
[building graphs in C++](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/cc/tutorials/example_trainer.cc).

We would like to support more client languages, as determined by community
interest. TensorFlow has a
[C-based client API](https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/core/public/tensor_c_api.h)
that makes it easy to build a client in many different languages. We invite
contributions of new language bindings.

#### Does TensorFlow make use of all the devices (GPUs and CPUs) available on my machine? <a class="md-anchor" id="AUTOGENERATED-does-tensorflow-make-use-of-all-the-devices--gpus-and-cpus--available-on-my-machine-"></a>

TensorFlow supports multiple GPUs and CPUs. See the how-to documentation on
[using GPUs with TensorFlow](../how_tos/using_gpu/index.md) for details of how
TensorFlow assigns operations to devices, and the
[CIFAR-10 tutorial](../tutorials/deep_cnn/index.md) for an example model that
uses multiple GPUs.

Note that TensorFlow only uses GPU devices with a compute capability greater
than 3.5.

#### Why does `Session.run()` hang when using a reader or a queue? <a class="md-anchor" id="AUTOGENERATED-why-does--session.run----hang-when-using-a-reader-or-a-queue-"></a>

The [reader](../api_docs/python/io_ops.md#ReaderBase) and
[queue](../api_docs/python/io_ops.md#QueueBase) classes provide special operations that
can *block* until input (or free space in a bounded queue) becomes
available. These operations allow you to build sophisticated
[input pipelines](../how_tos/reading_data/index.md), at the cost of making the
TensorFlow computation somewhat more complicated. See the how-to documentation
for
[using `QueueRunner` objects to drive queues and readers](../how_tos/reading_data/index.md#QueueRunners)
for more information on how to use them.

## Variables <a class="md-anchor" id="AUTOGENERATED-variables"></a>

See also the how-to documentation on [variables](../how_tos/variables/index.md)
and [variable scopes](../how_tos/variable_scope/index.md), and
[the API documentation for variables](../api_docs/python/state_ops.md).

#### What is the lifetime of a variable? <a class="md-anchor" id="AUTOGENERATED-what-is-the-lifetime-of-a-variable-"></a>

A variable is created when you first run the
[`tf.Variable.initializer`](../api_docs/python/state_ops.md#Variable.initializer)
operation for that variable in a session. It is destroyed when that
[`session is closed`](../api_docs/python/client.md#Session.close).

#### How do variables behave when they are concurrently accessed? <a class="md-anchor" id="AUTOGENERATED-how-do-variables-behave-when-they-are-concurrently-accessed-"></a>

Variables allow concurrent read and write operations. The value read from a
variable may change it is concurrently updated. By default, concurrent assigment
operations to a variable are allowed to run with no mutual exclusion. To acquire
a lock when assigning to a variable, pass `use_locking=True` to
[`Variable.assign()`](../api_docs/python/state_ops.md#Variable.assign).

## Tensor shapes <a class="md-anchor" id="AUTOGENERATED-tensor-shapes"></a>

See also the
[`TensorShape` API documentation](../api_docs/python/framework.md#TensorShape).

#### How can I determine the shape of a tensor in Python? <a class="md-anchor" id="AUTOGENERATED-how-can-i-determine-the-shape-of-a-tensor-in-python-"></a>

In TensorFlow, a tensor has both a static (inferred) shape and a dynamic (true)
shape. The static shape can be read using the
[`tf.Tensor.get_shape()`](../api_docs/python/framework.md#Tensor.get_shape)
method: this shape is inferred from the operations that were used to create the
tensor, and may be
[partially complete](../api_docs/python/framework.md#TensorShape). If the static
shape is not fully defined, the dynamic shape of a `Tensor` `t` can be
determined by evaluating [`tf.shape(t)`](../api_docs/python/array_ops.md#shape).

#### What is the difference between `x.set_shape()` and `x = tf.reshape(x)`? <a class="md-anchor" id="AUTOGENERATED-what-is-the-difference-between--x.set_shape----and--x---tf.reshape-x---"></a>

The [`tf.Tensor.set_shape()`](../api_docs/python/framework.md) method updates
the static shape of a `Tensor` object, and it is typically used to provide
additional shape information when this cannot be inferred directly. It does not
change the dynamic shape of the tensor.

The [`tf.reshape()`](../api_docs/python/array_ops.md#reshape) operation creates
a new tensor with a different dynamic shape.

#### How do I build a graph that works with variable batch sizes? <a class="md-anchor" id="AUTOGENERATED-how-do-i-build-a-graph-that-works-with-variable-batch-sizes-"></a>

It is often useful to build a graph that works with variable batch sizes, for
example so that the same code can be used for (mini-)batch training, and
single-instance inference. The resulting graph can be
[saved as a protocol buffer](../api_docs/python/framework.md#Graph.as_graph_def)
and
[imported into another program](../api_docs/python/framework.md#import_graph_def).

When building a variable-size graph, the most important thing to remember is not
to encode the batch size as a Python constant, but instead to use a symbolic
`Tensor` to represent it. The following tips may be useful:

* Use [`batch_size = tf.shape(input)[0]`](../api_docs/python/array_ops.md#shape)
  to extract the batch dimension from a `Tensor` called `input`, and store it in
  a `Tensor` called `batch_size`.

* Use [`tf.reduce_mean()`](../api_docs/python/math_ops.md#reduce_mean) instead
  of `tf.reduce_sum(...) / batch_size`.

* If you use
  [placeholders for feeding input](../how_tos/reading_data/index.md#Feeding),
  you can specify a variable batch dimension by creating the placeholder with
  [`tf.placeholder(..., shape=[None, ...])`](../api_docs/python/io_ops.md#placeholder). The
  `None` element of the shape corresponds to a variable-sized dimension.

## TensorBoard <a class="md-anchor" id="AUTOGENERATED-tensorboard"></a>

#### How can I visualize a TensorFlow graph? <a class="md-anchor" id="AUTOGENERATED-how-can-i-visualize-a-tensorflow-graph-"></a>

See the [graph visualization tutorial](../how_tos/graph_viz/index.md).

#### What is the simplest way to send data to TensorBoard? <a class="md-anchor" id="AUTOGENERATED-what-is-the-simplest-way-to-send-data-to-tensorboard-"></a>

Add summary ops to your TensorFlow graph, and use a
[`SummaryWriter`](../api_docs/python/train.md#SummaryWriter) to write
these summaries to a log directory.  Then, start TensorBoard using

    python tensorflow/tensorboard/tensorboard.py --logdir=path/to/log-directory

For more details, see the [Summaries and TensorBoard tutorial]
(../how_tos/summaries_and_tensorboard/index.md).

## 扩展 TensorFlow <a class="md-anchor" id="AUTOGENERATED-extending-tensorflow"></a>

See also the how-to documentation for
[adding a new operation to TensorFlow](../how_tos/adding_an_op/index.md).

#### My data is in a custom format. How do I read it using TensorFlow? <a class="md-anchor" id="AUTOGENERATED-my-data-is-in-a-custom-format.-how-do-i-read-it-using-tensorflow-"></a>

There are two main options for dealing with data in a custom format.

The easier option is to write parsing code in Python that transforms the data
into a numpy array, then feed a [`tf.placeholder()`]
(../api_docs/python/io_ops.md#placeholder) a tensor with that data. See the
documentation on
[using placeholders for input](../how_tos/reading_data/index.md#Feeding) for
more details. This approach is easy to get up and running, but the parsing can
be a performance bottleneck.

The more efficient option is to
[add a new op written in C++](../how_tos/adding_an_op/index.md) that parses your
data format. The
[guide to handling new data formats](../how_tos/new_data_formats/index.md) has
more information about the steps for doing this.

#### How do I define an operation that takes a variable number of inputs? <a class="md-anchor" id="AUTOGENERATED-how-do-i-define-an-operation-that-takes-a-variable-number-of-inputs-"></a>

The TensorFlow op registration mechanism allows you to define inputs that are a
single tensor, a list of tensors with the same type (for example when adding
together a variable-length list of tensors), or a list of tensors with different
types (for example when enqueuing a tuple of tensors to a queue).  See the
how-to documentation for
[adding an op with a list of inputs or outputs](../how_tos/adding_an_op/index.md#list-input-output)
for more details of how to define these different input types.

## 其他问题 <a class="md-anchor" id="AUTOGENERATED-miscellaneous"></a>

#### TensorFlow 能使用 Python 3 吗？ <a class="md-anchor" id="AUTOGENERATED-does-tensorflow-work-with-python-3-"></a>

我们只用 Python 2.7 进行了测试。我们了解对 Python 3 的兼容性来说，还需要有一些修改，欢迎大家朝这个方向多努力。

#### TensorFlow 的代码风格有什么规则？ <a class="md-anchor" id="AUTOGENERATED-what-is-tensorflow-s-coding-style-convention-"></a>

TensorFlow Python API 遵循 [PEP8](https://www.python.org/dev/peps/pep-0008/) 惯例。
<sup>*</sup> 特别的，我们使用 `CamelCase` 格式作为类名， `snake_case` 格式作为方程名， 方法名， 和属性名。我们也遵循
[Google Python style guide](https://google.github.io/styleguide/pyguide.html)。

TensorFlow C++ 代码遵循 [Google C++ style guide](http://google.github.io/styleguide/cppguide.html)。

(<sup>*</sup> 有一条例外: 我们使用 2 空格缩进而不是 4 空格缩进)