# 递归神经网络 <a class="md-anchor" id="AUTOGENERATED-recurrent-neural-networks"></a>

## 介绍 <a class="md-anchor" id="AUTOGENERATED-introduction"></a>

可以在 [this great article](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 看看递归神经网络特别是 LSTM 的介绍。

## 语言模型 <a class="md-anchor" id="AUTOGENERATED-language-modeling"></a>

此教程将展示如何在高任务难度的语言模型中训练递归神经网络。该问题的目标是调整一个概率模型以将概率分配到句柄。这实际上是通过预测给出了之前的词语历史记录的文本的接下来的词语来做到的。为此，我们将使用 PTB(Penn Tree Bank) 数据集，这是一种流行的用于测量这些模型的质量的基准，同时还具有小型化和相对的训练快速的的特点。

语言模型是许多诸如语音识别，机器翻译或图像字幕等有趣的难题的关键所在。没错，这真的很有意思--
可以参看 [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。

基于此目标，本教程将重现 [Zaremba et al., 2014](http://arxiv.org/abs/1409.2329) 的成果，该成果是应用 PTB 数据集得到的很棒的结果。

## 教程文件 <a class="md-anchor" id="AUTOGENERATED-tutorial-files"></a>

本教程使用的下面的文件引用自 `models/rnn/ptb`:

文件 | 作用
--- | ---
`ptb_word_lm.py` | 在 PTB 数据集上训练一个语言模型.
`reader.py` | 读取数据集.

## 下载及准备数据 <a class="md-anchor" id="AUTOGENERATED-download-and-prepare-the-data"></a>

本教程需要的数据在 data/ 路径下，其是 Tomas Mikolov 的网站上的的 PTB 数据集 http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz。

该数据集已经预先处理过并且包含了全部的 10000 个不同的词语，其中包括语句结束标记符以及针对稀有词语的特殊符号 (\<unk\>) 。我们把所有 `reader.py` 中的词语转换成唯一整型标识符，使其易于神经网络处理。

## 模型 <a class="md-anchor" id="AUTOGENERATED-the-model"></a>

### LSTM <a class="md-anchor" id="AUTOGENERATED-lstm"></a>

模型的核心由一个 LSTM 单元组成，The core of the model consists of an LSTM cell that processes one word at the
time and computes probabilities of the possible continuations of the sentence.
网络的存储状态由一个零矢量初始化并在读取每一个词语后更新。The memory state of the network is initialized with a vector of zeros and gets
updated after reading each word. Also, for computational reasons, we will
process data in mini-batches of size `batch_size`.

基础的伪代码就像下面这样：The basic pseudocode looks as follows:

```python
lstm = rnn_cell.BasicLSTMCell(lstm_size)
# 初始化 LSTM 存储状态Initial state of the LSTM memory.
state = tf.zeros([batch_size, lstm.state_size])

loss = 0.0
for current_batch_of_words in words_in_dataset:
    # 每次处理一批词语后更新状态值The value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # LSTM 输出可用于产生下一个词语的预测The LSTM output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities = tf.nn.softmax(logits)
    loss += loss_function(probabilities, target_words)
```

### 截断反向传播 <a class="md-anchor" id="AUTOGENERATED-truncated-backpropagation"></a>

为使学习过程易于处理，通常的做法是将反向传播的梯度截断成展开步骤的一个固定数字(`num_steps`)。In order to make the learning process tractable, it is a common practice to
truncate the gradients for backpropagation to a fixed number (`num_steps`)
of unrolled steps.
通过一次提供长度为 `num_steps` 的输入和每次迭代之后进行向后传递，这会很容易实现。This is easy to implement by feeding inputs of length `num_steps` at a time and
doing backward pass after each iteration.

一个简化版的用于图形创建的截断反向传播代码：A simplifed version of the code for the graph creation for truncated
backpropagation:

```python
# 一次给定的迭代中的输入占位符Placeholder for the inputs in a given iteration.
words = tf.placeholder(tf.int32, [batch_size, num_steps])

lstm = rnn_cell.BasicLSTMCell(lstm_size)
# 初始化 LSTM 存储状态Initial state of the LSTM memory.
initial_state = state = tf.zeros([batch_size, lstm.state_size])

for i in range(len(num_steps)):
    # 每次处理一批词语后更新状态值The value of state is updated after processing each batch of words.
    output, state = lstm(words[:, i], state)

    # 其余的代码。。。The rest of the code.
    # ...

final_state = state
```

下面展现如何实现迭代整个数据集：And this is how to implement an iteration over the whole dataset:

```python
# 一个 numpy 数组，保存每一批词语之后的 LSTM 状态A numpy array holding the state of LSTM after each batch of words.
numpy_state = initial_state.eval()
total_loss = 0.0
for current_batch_of_words in words_in_dataset:
    numpy_state, current_loss = session.run([final_state, loss],
        # 初始化来自上一次迭代的 LSTM 状态Initialize the LSTM state from the previous iteration.
        feed_dict={initial_state: numpy_state, words: current_batch_of_words})
    total_loss += current_loss
```

### 输入Inputs <a class="md-anchor" id="AUTOGENERATED-inputs"></a>

在提供给 LSTM 前，IDs 将被嵌入到一个密集的表示中(查看 [矢量表示教程](../../tutorials/word2vec/index.md))。The word IDs will be embedded into a dense representation (see the
[Vector Representations Tutorial](../../tutorials/word2vec/index.md)) before feeding to
the LSTM. 这种方式允许模型高效地表现特定词语的知识。This allows the model to efficiently represent the knowledge about
particular words. 代码也很容易写：It is also easy to write:

```python
# embedding_matrix 是形状的张量 is a tensor of shape [vocabulary_size, embedding size]
word_embeddings = tf.nn.embedding_lookup(embedding_matrix, word_ids)
```

嵌入的矩阵会被随机地初始化，模型将仅通过看一眼数据就学会区分词语的意思。The embedding matrix will be initialized randomly and the model will learn to
differentiate the meaning of words just by looking at the data.

### Loss Fuction <a class="md-anchor" id="AUTOGENERATED-loss-fuction"></a>

我们想使目标词语的平均负对数概率最小
```math
\text{loss} = -\frac{1}{N}\sum_{i=1}^{N} \ln p_{\text{target}_i}
```

实现起来并非很难，但是这里已经有了可用的函数 `sequence_loss_by_example` ，可以直接在这里使用。It is not very difficult to implement but the function
`sequence_loss_by_example` is already available, so we can just use it here.

文献报告中的典型方法是平均每个词语的困惑度，计算式为The typical measure reported in the papers is average per-word perplexity (often
just called perplexity), which is equal to

```math
e^{-\frac{1}{N}\sum_{i=1}^{N} \ln p_{\text{target}_i}} = e^{\text{loss}}
```

同时我们会监视训练过程中的困惑度值。and we will monitor its value throughout the training process.

### 多 LSTM 堆叠Stacking multiple LSTMs <a class="md-anchor" id="AUTOGENERATED-stacking-multiple-lstms"></a>

要想给模型更多的表单能力，可以添加多层 LSTM 来处理数据。To give the model more expressive power, we can add multiple layers of LSTMs
to process the data. 第一层的输出作为第二层的输入，以此类推。The output of the first layer will become the input of
the second and so on.

类 `MultiRNNCell` 可以无缝的将其实现。We have a class called `MultiRNNCell` that makes the implementation seamless:

```python
lstm = rnn_cell.BasicLSTMCell(lstm_size)
stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)

initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)
for i in range(len(num_steps)):
    # 每次处理一批词语后更新状态值The value of state is updated after processing each batch of words.
    output, state = stacked_lstm(words[:, i], state)

    # 其余的代码。。。The rest of the code.
    # ...

final_state = state
```

## 编译并运行代码 <a class="md-anchor" id="AUTOGENERATED-compile-and-run-the-code"></a>

首先需要构建库，在 CPU 上编译：

```
bazel build -c opt tensorflow/models/rnn/ptb:ptb_word_lm
```

如果你有一个强大的 GPU，可以运行：

```
bazel build -c opt --config=cuda tensorflow/models/rnn/ptb:ptb_word_lm
```

运行模型：

```
bazel-bin/tensorflow/models/rnn/ptb/ptb_word_lm \
  --data_path=/tmp/simple-examples/data/ --alsologtostderr --model small
```

教程代码中有 3 个支持的模型配置："small"，
"medium" 和 "large"。There are 3 supported model configurations in the tutorial code: "small",
"medium" and "large". 它们的不同有 LSTM 的大小，以及用于训练的超参数集。The difference between them is in size of the LSTMs and
the set of hyperparameters used for training.

模型越大，得到的结果应该更好。The larger the model, the better results it should get.在测试集中 `small` 模型应该可以达到低于 120 的困惑度，`large` 模型则是低于 80，考虑到它可能花费数小时来训练。The `small` model should
be able to reach perplexity below 120 on the test set and the `large` one below
80, though it might take several hours to train.

## 接下来是什么? <a class="md-anchor" id="AUTOGENERATED-what-next-"></a>

还有几个优化模型的技巧没有提到，包括：

* 降低学习曲线decreasing learning rate schedule,
* dropout between the LSTM layers.

学习和更改代码以进一步改善模型。
